#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BERT åˆ†é¡å™¨æ¨è«–åŸ·è¡Œè…³æœ¬ - ç°¡åŒ–ç‰ˆï¼ˆä½¿ç”¨çµ•å°è·¯å¾‘ï¼‰
æ ¹æ“š data_game_2025/docs/bert/inference.md PRD å¯¦ä½œ
"""

import os
import sys
import argparse
import pandas as pd
from datetime import datetime
from typing import List, Dict

# æ·»åŠ ç•¶å‰ç›®éŒ„åˆ° Python è·¯å¾‘
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

# æ·»åŠ  src ç›®éŒ„åˆ° Python è·¯å¾‘
src_dir = os.path.abspath(os.path.join(current_dir, '..'))
if src_dir not in sys.path:
    sys.path.insert(0, src_dir)

try:
    from bert_model.bert_travel_inference import TravelBertInference
    from bert_model.bert_name_inference import NameBertInference
    from bert_model.bert_config import BertConfig
    from utils import load_input_csv
except ImportError as e:
    print(f"âš ï¸ ç„¡æ³•è¼‰å…¥æ¨¡çµ„: {e}")
    print("è«‹ç¢ºèªæ‰€æœ‰ç›¸é—œæ¨¡çµ„å·²æ­£ç¢ºå®‰è£")
    sys.exit(1)


def infer_travel_classifier(
    model_dir: str,
    input_csv: str,
    batch_size: int = 32
) -> List[Dict]:
    """
    åŸ·è¡Œæ—…éŠåˆ†é¡æ¨è«–
    
    Args:
        model_dir: æ—…éŠåˆ†é¡æ¨¡å‹ç›®éŒ„
        input_csv: è¼¸å…¥ CSV æª”æ¡ˆè·¯å¾‘
        batch_size: æ‰¹æ¬¡å¤§å°
        
    Returns:
        é æ¸¬çµæœåˆ—è¡¨ï¼ŒåŒ…å« sms_id, travel_prob, label
    """
    print("=== BERT æ—…éŠç°¡è¨Šåˆ†é¡å™¨æ¨è«– ===")
    
    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆå¦‚æœæ˜¯æœ¬åœ°è·¯å¾‘ï¼‰æˆ–æ˜¯å¦ç‚º Hugging Face æ¨¡å‹ ID
    is_local_path = os.path.exists(model_dir) or os.path.isabs(model_dir)
    is_hf_model = not is_local_path and "/" in model_dir
    
    if not is_local_path and not is_hf_model:
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ—…éŠåˆ†é¡æ¨¡å‹: {model_dir}")
    
    # è¼‰å…¥æ¨è«–å™¨
    try:
        inferencer = TravelBertInference(model_dir)
        if is_hf_model:
            print(f"âœ… æ—…éŠåˆ†é¡æ¨¡å‹å¾ Hugging Face è¼‰å…¥æˆåŠŸ: {model_dir}")
        else:
            print(f"âœ… æ—…éŠåˆ†é¡æ¨¡å‹å¾æœ¬åœ°è¼‰å…¥æˆåŠŸ: {model_dir}")
    except Exception as e:
        raise Exception(f"è¼‰å…¥æ—…éŠåˆ†é¡æ¨¡å‹å¤±æ•—: {e}")
    
    # è¼‰å…¥è³‡æ–™
    try:
        messages = load_input_csv(input_csv)
        sms_ids = [msg['id'] for msg in messages]
        texts = [msg['message'] for msg in messages]
        print(f"âœ… è¼‰å…¥ {len(texts)} ç­†ç°¡è¨Š")
        
        # èª¿è©¦ï¼šæª¢æŸ¥å‰å¹¾å€‹ sms_id æ˜¯å¦æ­£ç¢º
        if len(sms_ids) > 0:
            print(f"ğŸ” æ—…éŠåˆ†é¡å‰5å€‹ sms_id: {sms_ids[:5]}")
            print(f"ğŸ” æ—…éŠåˆ†é¡ sms_id é¡å‹: {type(sms_ids[0])}")
        
    except Exception as e:
        raise Exception(f"è¼‰å…¥è¼¸å…¥æª”æ¡ˆå¤±æ•—: {e}")
    
    # æ‰¹æ¬¡æ¨è«–
    try:
        print(f"ğŸ”„ é–‹å§‹æ—…éŠåˆ†é¡æ¨è«–ï¼ˆæ‰¹æ¬¡å¤§å°: {batch_size}ï¼‰...")
        labels, probabilities = inferencer.predict_batch(texts, batch_size=batch_size)
        print(f"âœ… æ—…éŠåˆ†é¡æ¨è«–å®Œæˆ")
    except Exception as e:
        raise Exception(f"æ—…éŠåˆ†é¡æ¨è«–å¤±æ•—: {e}")
    
    # æ•´ç†çµæœï¼Œæ©Ÿç‡ä¿ç•™åˆ°å°æ•¸é»ç¬¬4ä½ä»¥é¿å…éåº¦å››æ¨äº”å…¥
    results = []
    for sms_id, prob, label in zip(sms_ids, probabilities, labels):
        results.append({
            'sms_id': sms_id,
            'travel_prob': round(prob, 4),
            'label': label
        })
    return results


def infer_name_classifier(
    model_dir: str,
    input_csv: str,
    batch_size: int = 32
) -> List[Dict]:
    """
    åŸ·è¡Œå§“ååˆ†é¡æ¨è«–
    
    Args:
        model_dir: å§“ååˆ†é¡æ¨¡å‹ç›®éŒ„
        input_csv: è¼¸å…¥ CSV æª”æ¡ˆè·¯å¾‘
        batch_size: æ‰¹æ¬¡å¤§å°
        
    Returns:
        é æ¸¬çµæœåˆ—è¡¨ï¼ŒåŒ…å« sms_id, name_prob, name_flg
    """
    print("=== BERT å§“åç°¡è¨Šåˆ†é¡å™¨æ¨è«– ===")
    
    # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆå¦‚æœæ˜¯æœ¬åœ°è·¯å¾‘ï¼‰æˆ–æ˜¯å¦ç‚º Hugging Face æ¨¡å‹ ID
    is_local_path = os.path.exists(model_dir) or os.path.isabs(model_dir)
    is_hf_model = not is_local_path and "/" in model_dir
    
    if not is_local_path and not is_hf_model:
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°å§“ååˆ†é¡æ¨¡å‹: {model_dir}")
    
    # è¼‰å…¥æ¨è«–å™¨
    try:
        inferencer = NameBertInference(model_dir)
        if is_hf_model:
            print(f"âœ… å§“ååˆ†é¡æ¨¡å‹å¾ Hugging Face è¼‰å…¥æˆåŠŸ: {model_dir}")
        else:
            print(f"âœ… å§“ååˆ†é¡æ¨¡å‹å¾æœ¬åœ°è¼‰å…¥æˆåŠŸ: {model_dir}")
    except Exception as e:
        raise Exception(f"è¼‰å…¥å§“ååˆ†é¡æ¨¡å‹å¤±æ•—: {e}")
    
    # è¼‰å…¥è³‡æ–™
    try:
        messages = load_input_csv(input_csv)
        sms_ids = [msg['id'] for msg in messages]
        texts = [msg['message'] for msg in messages]
        print(f"âœ… è¼‰å…¥ {len(texts)} ç­†ç°¡è¨Š")
        
        # èª¿è©¦ï¼šæª¢æŸ¥å‰å¹¾å€‹ sms_id æ˜¯å¦æ­£ç¢º
        if len(sms_ids) > 0:
            print(f"ğŸ” å§“ååˆ†é¡å‰5å€‹ sms_id: {sms_ids[:5]}")
            print(f"ğŸ” å§“ååˆ†é¡ sms_id é¡å‹: {type(sms_ids[0])}")
        
    except Exception as e:
        raise Exception(f"è¼‰å…¥è¼¸å…¥æª”æ¡ˆå¤±æ•—: {e}")
    
    # æ‰¹æ¬¡æ¨è«–
    try:
        print(f"ğŸ”„ é–‹å§‹å§“ååˆ†é¡æ¨è«–ï¼ˆæ‰¹æ¬¡å¤§å°: {batch_size}ï¼‰...")
        labels, probabilities = inferencer.predict_batch(texts, batch_size=batch_size)
        print(f"âœ… å§“ååˆ†é¡æ¨è«–å®Œæˆ")
    except Exception as e:
        raise Exception(f"å§“ååˆ†é¡æ¨è«–å¤±æ•—: {e}")
    
    # æ•´ç†çµæœï¼Œæ©Ÿç‡ä¿ç•™åˆ°å°æ•¸é»ç¬¬4ä½ä»¥é¿å…éåº¦å››æ¨äº”å…¥
    results = []
    for sms_id, prob, label in zip(sms_ids, probabilities, labels):
        results.append({
            'sms_id': sms_id,
            'name_prob': round(prob, 4),
            'name_flg': label
        })
    return results


def save_dataframe_chunked(df: pd.DataFrame, output_path: str, chunk_size: int = 10000) -> None:
    """
    åˆ†æ‰¹å„²å­˜ DataFrame ä»¥é¿å…è¨˜æ†¶é«”å•é¡Œ
    
    Args:
        df: è¦å„²å­˜çš„ DataFrame
        output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        chunk_size: æ¯æ‰¹çš„å¤§å°ï¼Œé è¨­ 10000
    """
    total_rows = len(df)
    
    if total_rows <= chunk_size:
        # è³‡æ–™é‡å°ï¼Œç›´æ¥å„²å­˜
        df.to_csv(output_path, index=False, encoding='utf-8')
        print(f"âœ… çµæœå·²ä¿å­˜: {output_path}")
    else:
        # è³‡æ–™é‡å¤§ï¼Œåˆ†æ‰¹å„²å­˜
        print(f"ğŸ”„ è³‡æ–™é‡è¼ƒå¤§ ({total_rows} ç­†)ï¼Œåˆ†æ‰¹å„²å­˜ä¸­...")
        for i in range(0, total_rows, chunk_size):
            chunk_end = min(i + chunk_size, total_rows)
            chunk_df = df.iloc[i:chunk_end]
            
            # ç¬¬ä¸€æ‰¹åŒ…å«è¡¨é ­ï¼Œå¾ŒçºŒæ‰¹æ¬¡ä¸åŒ…å«è¡¨é ­
            mode = 'w' if i == 0 else 'a'
            header = i == 0
            
            chunk_df.to_csv(output_path, mode=mode, header=header, index=False, encoding='utf-8')
            print(f"  ğŸ’¾ å·²å„²å­˜ç¬¬ {i//chunk_size + 1} æ‰¹ ({i+1}-{chunk_end}/{total_rows})")
        
        print(f"âœ… çµæœå·²åˆ†æ‰¹ä¿å­˜: {output_path}")


def combine_inference_results(
    travel_results: List[Dict],
    name_results: List[Dict],
    output_csv: str
) -> str:
    """
    åˆä½µæ—…éŠèˆ‡å§“ååˆ†é¡çµæœï¼Œä¸¦è¼¸å‡ºç‚º CSV
    ä½¿ç”¨è¨˜æ†¶é«”å‹å–„çš„æ–¹å¼è™•ç†å¤§é‡è³‡æ–™
    
    Args:
        travel_results: æ—…éŠåˆ†é¡çµæœ
        name_results: å§“ååˆ†é¡çµæœ
        output_csv: è¼¸å‡º CSV æª”æ¡ˆè·¯å¾‘
        
    Returns:
        è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
    """
    import gc
    
    print("=== åˆä½µæ¨è«–çµæœ ===")
    print(f"ğŸ”„ è™•ç†æ—…éŠè³‡æ–™: {len(travel_results)} ç­†")
    print(f"ğŸ”„ è™•ç†å§“åè³‡æ–™: {len(name_results)} ç­†")
    
    # åˆ†æ‰¹åˆä½µä»¥é¿å…è¨˜æ†¶é«”å•é¡Œ
    chunk_size = 10000
    total_batches = max(len(travel_results), len(name_results))
    
    # å»ºç«‹è¼¸å‡ºç›®éŒ„
    output_dir = os.path.dirname(output_csv)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    # å¦‚æœè³‡æ–™é‡è¼ƒå°ï¼Œä½¿ç”¨åŸæœ¬çš„æ–¹å¼
    if total_batches <= chunk_size:
        print("ğŸ“¦ è³‡æ–™é‡è¼ƒå°ï¼Œä½¿ç”¨æ¨™æº–åˆä½µæ–¹å¼")
        
        # å°‡çµæœè½‰æ›ç‚º DataFrame
        travel_df = pd.DataFrame(travel_results)
        name_df = pd.DataFrame(name_results)
        
        # èª¿è©¦ï¼šæª¢æŸ¥æ¯å€‹ DataFrame çš„å…§å®¹
        print(f"ğŸ” æ—…éŠåˆ†é¡çµæœ DataFrame å½¢ç‹€: {travel_df.shape}")
        print(f"ğŸ” æ—…éŠåˆ†é¡çµæœå‰3è¡Œ:\n{travel_df.head(3)}")
        
        print(f"ğŸ” å§“ååˆ†é¡çµæœ DataFrame å½¢ç‹€: {name_df.shape}")
        print(f"ğŸ” å§“ååˆ†é¡çµæœå‰3è¡Œ:\n{name_df.head(3)}")
        
        # åˆä½µçµæœï¼ˆä»¥ sms_id ç‚ºéµï¼‰
        merged_df = pd.merge(
            travel_df,
            name_df,
            on='sms_id',
            how='outer'
        )
        print(f"âœ… å·²åˆä½µçµæœ")
        
        # èª¿è©¦ï¼šæª¢æŸ¥åˆä½µå¾Œçš„ DataFrame
        print(f"ğŸ” åˆä½µå¾Œ DataFrame å½¢ç‹€: {merged_df.shape}")
        print(f"ğŸ” åˆä½µå¾Œå‰3è¡Œ:\n{merged_df.head(3)}")
        
        # ç¢ºä¿æ¬„ä½é †åºç¬¦åˆ PRD è¦æ±‚: sms_id, travel_prob, label, name_prob, name_flg
        result_df = merged_df[['sms_id', 'travel_prob', 'label', 'name_prob', 'name_flg']]
        print(f"âœ… å·²ç¢ºä¿æ¬„ä½é †åºç¬¦åˆ PRD è¦æ±‚")

        # ä¿®æ­£SMS IDæ’åºå•é¡Œï¼šå°‡sms_idè½‰æ›ç‚ºæ•´æ•¸å¾Œæ’åº
        try:
            result_df['sms_id'] = pd.to_numeric(result_df['sms_id'], errors='coerce')
            result_df = result_df.sort_values('sms_id').reset_index(drop=True)
            print(f"âœ… å·²æŒ‰SMS IDæ•´æ•¸é †åºæ’åº")
        except Exception as e:
            print(f"âš ï¸ SMS IDæ’åºè­¦å‘Š: {e}")
        
        # èª¿è©¦ï¼šæª¢æŸ¥æœ€çµ‚çµæœ
        print(f"ğŸ” æœ€çµ‚çµæœ DataFrame å‰3è¡Œ:\n{result_df.head(3)}")
        
        # ä½¿ç”¨åˆ†æ‰¹å„²å­˜å‡½æ•¸
        save_dataframe_chunked(result_df, output_csv)
        
        # æ¸…ç†è¨˜æ†¶é«”
        del travel_df, name_df, merged_df, result_df
        gc.collect()
        
    else:
        print(f"ğŸ“¦ è³‡æ–™é‡è¼ƒå¤§ï¼Œä½¿ç”¨åˆ†æ‰¹åˆä½µæ–¹å¼ï¼ˆæ¯æ‰¹ {chunk_size} ç­†ï¼‰")
        
        # å»ºç«‹ sms_id åˆ°ç´¢å¼•çš„æ˜ å°„ä»¥åŠ é€ŸæŸ¥æ‰¾
        print("ğŸ”„ å»ºç«‹ç´¢å¼•æ˜ å°„...")
        travel_dict = {item['sms_id']: item for item in travel_results}
        name_dict = {item['sms_id']: item for item in name_results}
        
        # ç²å–æ‰€æœ‰ sms_id ä¸¦æ’åº
        all_sms_ids = set(travel_dict.keys()) | set(name_dict.keys())
        print(f"ğŸ“Š ç¸½å…±æœ‰ {len(all_sms_ids)} å€‹å”¯ä¸€çš„ SMS ID")
        
        # å°‡ sms_id è½‰æ›ç‚ºæ•¸å­—ä¸¦æ’åº
        try:
            sorted_sms_ids = sorted(all_sms_ids, key=lambda x: int(x))
            print("âœ… SMS ID å·²æŒ‰æ•¸å­—é †åºæ’åº")
        except (ValueError, TypeError):
            sorted_sms_ids = sorted(all_sms_ids)
            print("âš ï¸ SMS ID ç„¡æ³•è½‰æ›ç‚ºæ•¸å­—ï¼Œä½¿ç”¨å­—ä¸²æ’åº")
        
        # åˆ†æ‰¹è™•ç†ä¸¦ç›´æ¥å¯«å…¥æª”æ¡ˆ
        first_batch = True
        total_processed = 0
        
        for i in range(0, len(sorted_sms_ids), chunk_size):
            batch_sms_ids = sorted_sms_ids[i:i + chunk_size]
            batch_results = []
            
            for sms_id in batch_sms_ids:
                # å¾å…©å€‹å­—å…¸ä¸­ç²å–è³‡æ–™
                travel_data = travel_dict.get(sms_id, {})
                name_data = name_dict.get(sms_id, {})
                
                # åˆä½µè³‡æ–™
                combined_row = {
                    'sms_id': sms_id,
                    'travel_prob': travel_data.get('travel_prob', 0.0),
                    'label': travel_data.get('label', 0),
                    'name_prob': name_data.get('name_prob', 0.0),
                    'name_flg': name_data.get('name_flg', 0)
                }
                batch_results.append(combined_row)
            
            # å°‡æ‰¹æ¬¡çµæœè½‰æ›ç‚º DataFrame ä¸¦å¯«å…¥
            batch_df = pd.DataFrame(batch_results)
            
            # å¯«å…¥æª”æ¡ˆ
            mode = 'w' if first_batch else 'a'
            header = first_batch
            
            batch_df.to_csv(output_csv, mode=mode, header=header, index=False, encoding='utf-8')
            
            total_processed += len(batch_results)
            batch_num = (i // chunk_size) + 1
            total_batches_calc = (len(sorted_sms_ids) + chunk_size - 1) // chunk_size
            
            print(f"  ï¿½ å·²è™•ç†ç¬¬ {batch_num}/{total_batches_calc} æ‰¹ ({total_processed}/{len(sorted_sms_ids)})")
            
            # æ¸…ç†è¨˜æ†¶é«”
            del batch_df, batch_results
            gc.collect()
            
            first_batch = False
        
        print(f"âœ… åˆ†æ‰¹åˆä½µå®Œæˆï¼Œç¸½å…±è™•ç† {total_processed} ç­†è³‡æ–™")
        
        # æ¸…ç†è¨˜æ†¶é«”
        del travel_dict, name_dict, all_sms_ids, sorted_sms_ids
        gc.collect()
    
    print(f"ğŸ“Š ç¸½å…±è™•ç† {len(travel_results)} ç­†ç°¡è¨Š")
    
    # è¨ˆç®—çµ±è¨ˆæ•¸æ“šï¼ˆé¿å…é‡æ–°è¼‰å…¥æ•´å€‹æª”æ¡ˆï¼‰
    travel_positive = sum(1 for r in travel_results if r.get('label', 0) == 1)
    name_positive = sum(1 for r in name_results if r.get('name_flg', 0) == 1)
    
    print(f"ğŸ“Š æ—…éŠç›¸é—œç°¡è¨Š: {travel_positive} ç­†")
    print(f"ğŸ“Š å§“åç›¸é—œç°¡è¨Š: {name_positive} ç­†")
    print(f"âœ… åˆä½µçµæœå·²ä¿å­˜: {output_csv}")
    
    return output_csv


def get_default_paths():
    """ç²å–é»˜èªçš„çµ•å°æ–‡ä»¶è·¯å¾‘ï¼ˆå¾é…ç½®æª”æ¡ˆæˆ–é è¨­å€¼è®€å–ï¼‰"""
    try:
        # ç¢ºä¿ä½¿ç”¨æ­£ç¢ºçš„é…ç½®æª”æ¡ˆè·¯å¾‘
        config_path = os.path.join(os.path.dirname(__file__), "config.ini")
        config = BertConfig(config_path)
        inference_config = config.get_inference_config()
        
        print(f"ğŸ“‹ æˆåŠŸè¼‰å…¥é…ç½®æª”æ¡ˆ: {config_path}")
        print(f"ğŸ“¦ é…ç½®çš„æ‰¹æ¬¡å¤§å°: {inference_config['inference_batch_size']}")
        
        # ç›´æ¥å¾é…ç½®æª”æ¡ˆè®€å– output_dirï¼ˆåœ¨ inference å€æ®µï¼‰
        output_dir = config.config.get('inference', 'output_dir')
        
        return {
            'input': config.config.get('inference', 'input_file'),
            'travel_model': config.config.get('inference', 'travel_model_path'),
            'name_model': config.config.get('inference', 'name_model_path'),
            'output_dir': output_dir,
            'batch_size': inference_config['inference_batch_size']
        }
    except Exception as e:
        # å¦‚æœé…ç½®æª”æ¡ˆè¼‰å…¥å¤±æ•—ï¼Œä½¿ç”¨é è¨­è·¯å¾‘
        print(f"âš ï¸ é…ç½®æª”æ¡ˆè¼‰å…¥å¤±æ•—: {e}")
        print("ğŸ”„ ä½¿ç”¨é è¨­é…ç½®...")
        return {
            'input': 'data_game_2025/data/raw/datagame_sms_stage1_raw_TEXT_ONLY.csv',
            'travel_model': 'data_game_2025/results/travel_bert_20250725_2337',
            'name_model': 'data_game_2025/results/name_bert_20250725_2359',
            'output_dir': 'data_game_2025/data/results',
            'batch_size': 32
        }


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(
        description='BERT åˆ†é¡å™¨æ¨è«– - æ ¹æ“š PRD æ–‡ä»¶å¯¦ä½œ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¯„ä¾‹:
  # ä½¿ç”¨é è¨­è·¯å¾‘é€²è¡Œæ¨è«–
  python run_inference_simple.py
  
  # æŒ‡å®šæ¨¡å‹è·¯å¾‘é€²è¡Œæ¨è«–
  python run_inference_simple.py \\
    --input data_game_2025/data/raw/datagame_sms_stage1_raw_TEXT_ONLY.csv \\
    --travel-model data_game_2025/results/travel_bert_20250725_2337 \\
    --name-model data_game_2025/results/name_bert_20250725_2359 \\
    --output data_game_2025/data/results/inference_result.csv
  
  # åªåŸ·è¡Œæ—…éŠåˆ†é¡æ¨è«–
  python run_inference_simple.py --mode travel \\
    --travel-model data_game_2025/results/travel_bert_20250725_2337
        """
    )
    
    # æ¨¡å¼é¸æ“‡
    parser.add_argument(
        '--mode', 
        choices=['travel', 'name', 'both'], 
        default='both',
        help='æ¨è«–æ¨¡å¼: travelï¼ˆæ—…éŠï¼‰, nameï¼ˆå§“åï¼‰, æˆ– bothï¼ˆå…©è€…ï¼‰'
    )
    
    # è¼¸å…¥æª”æ¡ˆ
    parser.add_argument(
        '--input', 
        type=str,
        help='è¼¸å…¥ CSV æª”æ¡ˆè·¯å¾‘ï¼ˆé è¨­ä½¿ç”¨é…ç½®æª”æ¡ˆä¸­çš„è·¯å¾‘ï¼‰'
    )
    
    # æ¨¡å‹è·¯å¾‘
    parser.add_argument(
        '--travel-model', 
        type=str,
        help='æ—…éŠåˆ†é¡æ¨¡å‹ç›®éŒ„è·¯å¾‘'
    )
    
    parser.add_argument(
        '--name-model', 
        type=str,
        help='å§“ååˆ†é¡æ¨¡å‹ç›®éŒ„è·¯å¾‘'
    )
    
    # è¼¸å‡ºæª”æ¡ˆ
    parser.add_argument(
        '--output', 
        type=str,
        help='è¼¸å‡º CSV æª”æ¡ˆè·¯å¾‘ï¼ˆé è¨­ç‚ºæ™‚é–“æˆ³å‘½åï¼‰'
    )
    
    # æ¨è«–åƒæ•¸
    parser.add_argument(
        '--batch-size', 
        type=int, 
        help='æ‰¹æ¬¡å¤§å°ï¼ˆé è¨­å¾é…ç½®æª”æ¡ˆè®€å–ï¼‰'
    )
    
    args = parser.parse_args()
    
    # ç²å–é è¨­è·¯å¾‘
    print("ğŸ”§ è¼‰å…¥é…ç½®...")
    default_paths = get_default_paths()
    
    # è¨­å®šæ‰¹æ¬¡å¤§å°ï¼ˆå„ªå…ˆä½¿ç”¨å‘½ä»¤åˆ—åƒæ•¸ï¼Œå¦å‰‡ä½¿ç”¨é…ç½®æª”æ¡ˆï¼‰
    batch_size = args.batch_size if args.batch_size is not None else default_paths['batch_size']
    
    if args.batch_size is not None:
        print(f"ğŸ“¦ ä½¿ç”¨å‘½ä»¤åˆ—æŒ‡å®šçš„æ‰¹æ¬¡å¤§å°: {batch_size}")
    else:
        print(f"ğŸ“¦ ä½¿ç”¨é…ç½®æª”æ¡ˆçš„æ‰¹æ¬¡å¤§å°: {batch_size}")
    
    # è¨­å®šè¼¸å…¥æª”æ¡ˆè·¯å¾‘
    input_csv = args.input or default_paths['input']
    if not os.path.exists(input_csv):
        print(f"âŒ æ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ: {input_csv}")
        return 1
    
    # è¨­å®šè¼¸å‡ºæª”æ¡ˆè·¯å¾‘
    if args.output:
        output_csv = args.output
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        output_filename = f"both_multiligual_{timestamp}.csv"
        output_csv = os.path.join(default_paths['output_dir'], output_filename)
    
    print("ğŸš€ é–‹å§‹ BERT åˆ†é¡å™¨æ¨è«–æµç¨‹")
    print("=" * 50)
    print(f"ğŸ“„ è¼¸å…¥æª”æ¡ˆ: {input_csv}")
    print(f"ğŸ’¾ è¼¸å‡ºæª”æ¡ˆ: {output_csv}")
    print(f"ğŸ”§ æ¨è«–æ¨¡å¼: {args.mode}")
    print(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°: {batch_size}")
    print()
    
    results = {}
    
    try:
        # åŸ·è¡Œæ—…éŠåˆ†é¡æ¨è«–
        if args.mode in ['travel', 'both']:
            travel_model = args.travel_model or default_paths['travel_model']
            
            # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆæœ¬åœ°è·¯å¾‘ï¼‰æˆ–æ˜¯å¦ç‚º Hugging Face æ¨¡å‹ ID
            is_local_path = os.path.exists(travel_model) or os.path.isabs(travel_model)
            is_hf_model = not is_local_path and "/" in travel_model
            
            if not is_local_path and not is_hf_model:
                print(f"âŒ æ‰¾ä¸åˆ°æ—…éŠåˆ†é¡æ¨¡å‹: {travel_model}")
                return 1
            
            travel_results = infer_travel_classifier(
                model_dir=travel_model,
                input_csv=input_csv,
                batch_size=batch_size
            )
            results['travel'] = travel_results
            print(f"âœ… æ—…éŠåˆ†é¡æ¨è«–æˆåŠŸï¼Œè™•ç† {len(travel_results)} ç­†ç°¡è¨Š")
            print()
        
        # åŸ·è¡Œå§“ååˆ†é¡æ¨è«–
        if args.mode in ['name', 'both']:
            name_model = args.name_model or default_paths['name_model']
            
            # æª¢æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆæœ¬åœ°è·¯å¾‘ï¼‰æˆ–æ˜¯å¦ç‚º Hugging Face æ¨¡å‹ ID
            is_local_path = os.path.exists(name_model) or os.path.isabs(name_model)
            is_hf_model = not is_local_path and "/" in name_model
            
            if not is_local_path and not is_hf_model:
                print(f"âŒ æ‰¾ä¸åˆ°å§“ååˆ†é¡æ¨¡å‹: {name_model}")
                return 1
            
            name_results = infer_name_classifier(
                model_dir=name_model,
                input_csv=input_csv,
                batch_size=batch_size
            )
            results['name'] = name_results
            print(f"âœ… å§“ååˆ†é¡æ¨è«–æˆåŠŸï¼Œè™•ç† {len(name_results)} ç­†ç°¡è¨Š")
            print()
        
        # åˆä½µä¸¦è¼¸å‡ºçµæœ
        if args.mode == 'both':
            # å…©ç¨®åˆ†é¡éƒ½åŸ·è¡Œï¼Œåˆä½µçµæœ
            combine_inference_results(
                travel_results=results['travel'],
                name_results=results['name'],
                output_csv=output_csv
            )
        elif args.mode == 'travel':
            # åªæœ‰æ—…éŠåˆ†é¡ï¼Œç›´æ¥è¼¸å‡ºï¼ˆæ”¯æ´åˆ†æ‰¹å„²å­˜ï¼‰
            travel_df = pd.DataFrame(results['travel'])
            os.makedirs(os.path.dirname(output_csv), exist_ok=True)
            save_dataframe_chunked(travel_df, output_csv)
            
        elif args.mode == 'name':
            # åªæœ‰å§“ååˆ†é¡ï¼Œç›´æ¥è¼¸å‡ºï¼ˆæ”¯æ´åˆ†æ‰¹å„²å­˜ï¼‰
            name_df = pd.DataFrame(results['name'])
            os.makedirs(os.path.dirname(output_csv), exist_ok=True)
            save_dataframe_chunked(name_df, output_csv)
        
        # æ‰“å°çµæœç¸½çµ
        print("\n" + "=" * 50)
        print("ğŸ“Š æ¨è«–çµæœç¸½çµ:")
        
        if 'travel' in results:
            travel_positive = sum(1 for r in results['travel'] if r['label'] == 1)
            print(f"  ğŸ§³ æ—…éŠåˆ†é¡: {travel_positive}/{len(results['travel'])} ç­†é æ¸¬ç‚ºæ—…éŠç›¸é—œ")
        
        if 'name' in results:
            name_positive = sum(1 for r in results['name'] if r['name_flg'] == 1)
            print(f"  ğŸ‘¤ å§“ååˆ†é¡: {name_positive}/{len(results['name'])} ç­†é æ¸¬ç‚ºå§“åç›¸é—œ")
        
        print(f"  ğŸ’¾ çµæœæª”æ¡ˆ: {output_csv}")
        print("ğŸ‰ æ¨è«–å®Œæˆï¼")
        
        return 0
        
    except Exception as e:
        print(f"âŒ æ¨è«–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
        return 1


def print_usage_examples():
    """æ‰“å°ä½¿ç”¨ç¯„ä¾‹"""
    print("BERT åˆ†é¡å™¨æ¨è«–è…³æœ¬ä½¿ç”¨ç¯„ä¾‹:")
    print()
    print("1. ä½¿ç”¨é è¨­è·¯å¾‘é€²è¡Œé›™æ¨¡å‹æ¨è«–:")
    print("   python run_inference_simple.py")
    print()
    print("2. æŒ‡å®šæ¨¡å‹è·¯å¾‘é€²è¡Œæ¨è«–:")
    print("   python run_inference_simple.py \\")
    print("     --input data_game_2025/data/raw/datagame_sms_stage1_raw_TEXT_ONLY.csv \\")
    print("     --travel-model data_game_2025/results/travel_bert_20250725_2337 \\")
    print("     --name-model data_game_2025/results/name_bert_20250725_2359 \\")
    print("     --output data_game_2025/data/results/inference_result.csv")
    print()
    print("3. åªåŸ·è¡Œæ—…éŠåˆ†é¡æ¨è«–:")
    print("   python run_inference_simple.py --mode travel \\")
    print("     --travel-model data_game_2025/results/travel_bert_20250725_2337")
    print()
    print("4. åªåŸ·è¡Œå§“ååˆ†é¡æ¨è«–:")
    print("   python run_inference_simple.py --mode name \\")
    print("     --name-model data_game_2025/results/name_bert_20250725_2359")
    print()
    print("5. æŒ‡å®šæ‰¹æ¬¡å¤§å°:")
    print("   python run_inference_simple.py --batch-size 16")


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
