#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å…±ç”¨å·¥å…·å‡½æ•¸ (for ç°¡è¨Šåˆ†é¡ä»»å‹™)
"""
import os
import csv
from typing import Dict, List
import xml.etree.ElementTree as ET

def load_input_csv(file_path: str) -> List[Dict[str, str]]:
    """
    è¼‰å…¥è¼¸å…¥ CSV æª”æ¡ˆ (æ–°æ ¼å¼: sms_id, sms_body, label, name_flg)
    Args:
        file_path: CSV æª”æ¡ˆè·¯å¾‘
    Returns:
        ç°¡è¨Šåˆ—è¡¨ï¼Œæ ¼å¼ç‚º sms_id, sms_body
    """
    messages = []
    try:
        with open(file_path, 'r', encoding='utf-8-sig') as f:  # ä½¿ç”¨ utf-8-sig è™•ç† BOM
            reader = csv.DictReader(f)
            
            # èª¿è©¦ï¼šæª¢æŸ¥æ¬„ä½åç¨±
            fieldnames = reader.fieldnames
            print(f"ğŸ” CSV æ¬„ä½åç¨±: {fieldnames}")
            
            for row in reader:
                # æ”¯æ´å¤šç¨® ID å’Œå…§å®¹æ¬„ä½åç¨±ï¼Œä¸¦è™•ç†å¯èƒ½çš„ BOM å•é¡Œ
                sms_id = None
                sms_body = None
                
                # å°‹æ‰¾ ID æ¬„ä½ï¼ˆè™•ç†å¯èƒ½çš„ BOM æˆ–ç©ºç™½å­—å…ƒï¼‰
                for key in row.keys():
                    clean_key = key.strip().lstrip('\ufeff')  # ç§»é™¤ BOM å’Œç©ºç™½
                    if clean_key in ["sms_id", "id"]:
                        sms_id = row[key]
                        break
                
                # å°‹æ‰¾å…§å®¹æ¬„ä½
                for key in row.keys():
                    clean_key = key.strip().lstrip('\ufeff')
                    if clean_key in ["sms_body", "content", "message", "body"]:
                        sms_body = row[key]
                        break
                
                if sms_id and sms_body:
                    messages.append({
                        "id": sms_id.strip(),
                        "message": sms_body.strip()
                    })
                    
        print(f"âœ… load_input_csv æˆåŠŸè¼‰å…¥ {len(messages)} ç­†ç°¡è¨Š")
        
        # èª¿è©¦ï¼šæª¢æŸ¥å‰å¹¾ç­†è³‡æ–™
        if len(messages) > 0:
            print(f"ğŸ” load_input_csv å‰3ç­†è³‡æ–™:")
            for i, msg in enumerate(messages[:3]):
                print(f"    {i+1}. ID: '{msg['id']}' (é¡å‹: {type(msg['id'])})")
                print(f"       æ–‡å­—: '{msg['message'][:50]}...'")
        else:
            print("âš ï¸ æ²’æœ‰è¼‰å…¥ä»»ä½•è³‡æ–™ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆæ ¼å¼å’Œæ¬„ä½åç¨±")
                
    except FileNotFoundError:
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆ: {file_path}")
    except Exception as e:
        raise Exception(f"è®€å– CSV æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    return messages

def load_labeled_data(file_path: str, category_type: str) -> tuple:
    """
    è¼‰å…¥æ–°æ ¼å¼çš„æ¨™è¨»æ•¸æ“š
    Args:
        file_path: CSV æª”æ¡ˆè·¯å¾‘
        category_type: åˆ†é¡é¡å‹ ("name" æˆ– "travel")
    Returns:
        (texts, labels) å…ƒçµ„
    """
    import pandas as pd
    
    try:
        df = pd.read_csv(file_path)
        
        # æ ¹æ“šåˆ†é¡é¡å‹é¸æ“‡å°æ‡‰çš„æ¨™ç±¤æ¬„ä½
        if category_type == "name":
            # name åˆ†é¡ä½¿ç”¨ name_flg æ¬„ä½
            df = df[df['name_flg'].notna()].copy()
            texts = df['sms_body'].tolist()
            labels = df['name_flg'].astype(int).tolist()
        elif category_type == "travel":
            # travel åˆ†é¡ä½¿ç”¨ label æ¬„ä½
            df = df[df['label'].notna()].copy()
            texts = df['sms_body'].tolist()
            labels = df['label'].astype(int).tolist()
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„åˆ†é¡é¡å‹: {category_type}")
        
        return texts, labels
        
    except FileNotFoundError:
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨™è¨»æª”æ¡ˆ: {file_path}")
    except Exception as e:
        raise Exception(f"è®€å–æ¨™è¨»æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def load_multiple_datasets(file_paths: List[str], category_type: str) -> tuple:
    """
    è¼‰å…¥å¤šå€‹æ•¸æ“šé›†æª”æ¡ˆä¸¦åˆä½µ
    Args:
        file_paths: æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
        category_type: åˆ†é¡é¡å‹ ("name" æˆ– "travel")
    Returns:
        (all_texts, all_labels) å…ƒçµ„
    """
    all_texts = []
    all_labels = []
    
    for file_path in file_paths:
        if os.path.exists(file_path):
            texts, labels = load_labeled_data(file_path, category_type)
            all_texts.extend(texts)
            all_labels.extend(labels)
            print(f"è¼‰å…¥ {file_path}: {len(texts)} ç­†æ•¸æ“š")
        else:
            print(f"è­¦å‘Š: æª”æ¡ˆä¸å­˜åœ¨ {file_path}")
    
    print(f"ç¸½è¨ˆè¼‰å…¥ {len(all_texts)} ç­† {category_type} åˆ†é¡æ•¸æ“š")
    return all_texts, all_labels

def load_labeled_data_simple_with_ids(file_path: str, category_type: str) -> tuple:
    """
    ç°¡åŒ–ç‰ˆè¼‰å…¥æ¨™è¨»æ•¸æ“šï¼ŒåŒ…å« SMS IDï¼ˆç›´æ¥ä½¿ç”¨çµ•å°è·¯å¾‘ï¼‰
    Args:
        file_path: æ•¸æ“šæª”æ¡ˆçµ•å°è·¯å¾‘
        category_type: åˆ†é¡é¡å‹ ("name" æˆ– "travel")
    Returns:
        (texts, labels, sms_ids) å…ƒçµ„
    """
    import pandas as pd
    
    try:
        df = pd.read_csv(file_path)
        
        # æ ¹æ“šåˆ†é¡é¡å‹é¸æ“‡å°æ‡‰çš„æ¨™ç±¤æ¬„ä½
        if category_type == "name":
            # name åˆ†é¡ä½¿ç”¨ name_flg æ¬„ä½
            df = df[df['name_flg'].notna()].copy()
            texts = df['sms_body'].tolist()
            labels = df['name_flg'].astype(int).tolist()
            sms_ids = df['sms_id'].astype(str).tolist()
        elif category_type == "travel":
            # travel åˆ†é¡ä½¿ç”¨ label æ¬„ä½
            df = df[df['label'].notna()].copy()
            texts = df['sms_body'].tolist()
            labels = df['label'].astype(int).tolist()
            sms_ids = df['sms_id'].astype(str).tolist()
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„åˆ†é¡é¡å‹: {category_type}")
        
        print(f"å¾ {file_path} è¼‰å…¥ {len(texts)} ç­† {category_type} æ•¸æ“š")
        return texts, labels, sms_ids
        
    except FileNotFoundError:
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {file_path}")
    except Exception as e:
        raise Exception(f"è®€å–æª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def load_multiple_datasets_simple_with_ids(file_paths: List[str], category_type: str) -> tuple:
    """
    è¼‰å…¥å¤šå€‹æ•¸æ“šé›†æª”æ¡ˆä¸¦åˆä½µï¼ŒåŒ…å« SMS IDï¼ˆç°¡åŒ–ç‰ˆï¼‰
    Args:
        file_paths: æª”æ¡ˆçµ•å°è·¯å¾‘åˆ—è¡¨
        category_type: åˆ†é¡é¡å‹ ("name" æˆ– "travel")
    Returns:
        (all_texts, all_labels, all_sms_ids) å…ƒçµ„
    """
    all_texts = []
    all_labels = []
    all_sms_ids = []
    
    for file_path in file_paths:
        if os.path.exists(file_path):
            texts, labels, sms_ids = load_labeled_data_simple_with_ids(file_path, category_type)
            all_texts.extend(texts)
            all_labels.extend(labels)
            all_sms_ids.extend(sms_ids)
        else:
            print(f"è­¦å‘Š: æª”æ¡ˆä¸å­˜åœ¨ {file_path}")
    
    print(f"ç¸½è¨ˆè¼‰å…¥ {len(all_texts)} ç­† {category_type} åˆ†é¡æ•¸æ“š")
    return all_texts, all_labels, all_sms_ids

def load_labeled_data_simple(file_path: str, category_type: str) -> tuple:
    """
    ç°¡åŒ–ç‰ˆè¼‰å…¥æ¨™è¨»æ•¸æ“šï¼ˆç›´æ¥ä½¿ç”¨çµ•å°è·¯å¾‘ï¼‰
    Args:
        file_path: æ•¸æ“šæª”æ¡ˆçµ•å°è·¯å¾‘
        category_type: åˆ†é¡é¡å‹ ("name" æˆ– "travel")
    Returns:
        (texts, labels) å…ƒçµ„
    """
    import pandas as pd
    
    try:
        df = pd.read_csv(file_path)
        
        # æ ¹æ“šåˆ†é¡é¡å‹é¸æ“‡å°æ‡‰çš„æ¨™ç±¤æ¬„ä½
        if category_type == "name":
            # name åˆ†é¡ä½¿ç”¨ name_flg æ¬„ä½
            df = df[df['name_flg'].notna()].copy()
            texts = df['sms_body'].tolist()
            labels = df['name_flg'].astype(int).tolist()
        elif category_type == "travel":
            # travel åˆ†é¡ä½¿ç”¨ label æ¬„ä½
            df = df[df['label'].notna()].copy()
            texts = df['sms_body'].tolist()
            labels = df['label'].astype(int).tolist()
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„åˆ†é¡é¡å‹: {category_type}")
        
        print(f"å¾ {file_path} è¼‰å…¥ {len(texts)} ç­† {category_type} æ•¸æ“š")
        return texts, labels
        
    except FileNotFoundError:
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {file_path}")
    except Exception as e:
        raise Exception(f"è®€å–æª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def load_multiple_datasets_simple(file_paths: List[str], category_type: str) -> tuple:
    """
    è¼‰å…¥å¤šå€‹æ•¸æ“šé›†æª”æ¡ˆä¸¦åˆä½µï¼ˆç°¡åŒ–ç‰ˆï¼‰
    Args:
        file_paths: æª”æ¡ˆçµ•å°è·¯å¾‘åˆ—è¡¨
        category_type: åˆ†é¡é¡å‹ ("name" æˆ– "travel")
    Returns:
        (all_texts, all_labels) å…ƒçµ„
    """
    all_texts = []
    all_labels = []
    
    for file_path in file_paths:
        if os.path.exists(file_path):
            texts, labels = load_labeled_data_simple(file_path, category_type)
            all_texts.extend(texts)
            all_labels.extend(labels)
        else:
            print(f"è­¦å‘Š: æª”æ¡ˆä¸å­˜åœ¨ {file_path}")
    
    print(f"ç¸½è¨ˆè¼‰å…¥ {len(all_texts)} ç­† {category_type} åˆ†é¡æ•¸æ“š")
    return all_texts, all_labels

def load_multiple_datasets(file_paths: List[str], category_type: str) -> tuple:
    """
    è¼‰å…¥å¤šå€‹æ•¸æ“šé›†æª”æ¡ˆä¸¦åˆä½µ (ä½¿ç”¨çµ•å°è·¯å¾‘)
    Args:
        file_paths: æª”æ¡ˆçµ•å°è·¯å¾‘åˆ—è¡¨
        category_type: åˆ†é¡é¡å‹ ("name" æˆ– "travel")
    Returns:
        (all_texts, all_labels) å…ƒçµ„
    """
    all_texts = []
    all_labels = []
    
    for file_path in file_paths:
        if os.path.exists(file_path):
            try:
                texts, labels = load_labeled_data(file_path, category_type)
                all_texts.extend(texts)
                all_labels.extend(labels)
                print(f"è¼‰å…¥ {file_path}: {len(texts)} ç­†æ•¸æ“š")
            except Exception as e:
                print(f"è¼‰å…¥å¤±æ•— {file_path}: {e}")
        else:
            print(f"è­¦å‘Š: æª”æ¡ˆä¸å­˜åœ¨ {file_path}")
    
    print(f"ç¸½è¨ˆè¼‰å…¥ {len(all_texts)} ç­† {category_type} åˆ†é¡æ•¸æ“š")
    return all_texts, all_labels

def save_results_csv(results: Dict[str, int], output_path: str, classifier_type: str = "name") -> None:
    """
    å„²å­˜åˆ†é¡çµæœåˆ° CSV æª”æ¡ˆï¼Œä½¿ç”¨ç«¶è³½è¦å‰‡çš„æ¬„ä½åç¨±
    Args:
        results: åˆ†é¡çµæœå­—å…¸ {id: result}
        output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        classifier_type: åˆ†é¡å™¨é¡å‹ ("name" æˆ– "travel")
    """
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    try:
        with open(output_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            # æ ¹æ“šåˆ†é¡å™¨é¡å‹æ±ºå®šæ¬„ä½åç¨±
            if classifier_type == "name":
                writer.writerow(['sms_id', 'name_flg'])
            else:  # travel
                writer.writerow(['sms_id', 'label'])
            
            for k, v in results.items():
                writer.writerow([k, v])
        print(f"åˆ†é¡çµæœå·²å„²å­˜è‡³: {output_path}")
    except Exception as e:
        raise Exception(f"å„²å­˜ CSV æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

def parse_xml_response(xml_response: str, tag: str) -> Dict[str, int]:
    """
    è§£æ LLM è¿”å›çš„ XML æ ¼å¼å›æ‡‰
    Args:
        xml_response: LLM è¿”å›çš„ XML æ ¼å¼å­—ä¸²
        tag: çµæœæ¨™ç±¤ (å¦‚ isName, isTravel)
    Returns:
        å­—å…¸æ ¼å¼ {id: 0/1}
    """
    results = {}
    try:
        cleaned_response = xml_response.strip()
        if cleaned_response.startswith("```xml"):
            cleaned_response = cleaned_response[6:]
        if cleaned_response.endswith("```"):
            cleaned_response = cleaned_response[:-3]
        cleaned_response = cleaned_response.strip()
        root = ET.fromstring(cleaned_response)
        for message in root.findall('message'):
            mid = message.attrib.get('id')
            result = message.find(tag)
            if mid is not None and result is not None:
                results[mid] = 1 if result.text.strip().lower() == 'yes' else 0
    except ET.ParseError as e:
        print(f"XML è§£æéŒ¯èª¤: {e}")
        print(f"å›æ‡‰å…§å®¹: {xml_response}")
        raise
    except Exception as e:
        print(f"è§£æå›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        raise
    return results

def estimate_token_count(prompt: str, messages: List[Dict[str, str]]) -> int:
    """
    ç²—ç•¥ä¼°ç®— prompt + messages çš„ token æ•¸ï¼ˆä»¥å­—å…ƒæ•¸/4 è¿‘ä¼¼ï¼‰
    """
    total_chars = len(prompt)
    for msg in messages:
        total_chars += len(msg.get('message', ''))
    return total_chars // 4

def escape_xml(text: str) -> str:
    """
    å°‡æ–‡å­—ä¸­çš„ XML ç‰¹æ®Šå­—å…ƒé€²è¡Œè½‰ç¾©
    """
    if not text:
        return text
    text = text.replace("&", "&amp;")
    text = text.replace("<", "&lt;")
    text = text.replace(">", "&gt;")
    text = text.replace('"', "&quot;")
    text = text.replace("'", "&apos;")
    return text

def sanitize_llm_xml(xml_str: str) -> str:
    """
    ä¿®å¾© LLM è¼¸å‡ºçš„å¸¸è¦‹ XML æ ¼å¼éŒ¯èª¤
    """
    import re
    if not xml_str:
        return xml_str
    xml_str = re.sub(r'^```xml\s*', '', xml_str, flags=re.MULTILINE)
    xml_str = re.sub(r'\s*```\s*$', '', xml_str, flags=re.MULTILINE)
    seen_ids = set()
    lines = xml_str.split('\n')
    filtered_lines = []
    skip_until_close = False
    for line in lines:
        match = re.search(r'<message id=\"([^\"]+)\">', line)
        if match:
            msg_id = match.group(1)
            if msg_id in seen_ids:
                skip_until_close = True
                continue
            else:
                seen_ids.add(msg_id)
                skip_until_close = False
        if '</message>' in line and skip_until_close:
            skip_until_close = False
            continue
        if not skip_until_close:
            filtered_lines.append(line)
    return '\n'.join(filtered_lines)

def get_existing_classified_ids(output_dir: str, prefix: str, model_name: str) -> set:
    """
    è®€å–åŒé¡å‹åˆ†é¡çµæœæª”æ¡ˆï¼Œå–å¾—å·²åˆ†é¡çš„ ID æ¸…å–®
    Args:
        output_dir: è¼¸å‡ºç›®éŒ„è·¯å¾‘
        prefix: æª”æ¡ˆå‰ç¶´ï¼ˆå¦‚ name æˆ– travelï¼‰
        model_name: æ¨¡å‹åç¨±
    Returns:
        å·²åˆ†é¡çš„ ID é›†åˆ
    """
    import glob
    import csv
    from pathlib import Path
    existing_ids = set()
    safe_model_name = model_name.replace("/", "_").replace(":", "_")
    pattern = f"{prefix}_{safe_model_name}_*.csv"
    search_path = Path(output_dir) / pattern
    try:
        matching_files = glob.glob(str(search_path))
        for file_path in matching_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        # æ”¯æ´å¤šç¨® id æ¬„ä½åç¨±
                        sms_id = row.get('sms_id') or row.get('id')
                        if sms_id:
                            existing_ids.add(str(sms_id))
            except Exception as e:
                print(f"è®€å–æª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue
        if existing_ids:
            print(f"æ‰¾åˆ° {len(matching_files)} å€‹å·²å­˜åœ¨çš„åˆ†é¡æª”æ¡ˆï¼Œå…± {len(existing_ids)} ç­†å·²åˆ†é¡è¨˜éŒ„")
    except Exception as e:
        print(f"æœå°‹å·²å­˜åœ¨æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    return existing_ids

def clean_text_for_bert(text: str) -> str:
    """
    ç‚º BERT æ¨¡å‹æ¸…ç†æ–‡æœ¬
    Args:
        text: åŸå§‹æ–‡æœ¬
    Returns:
        æ¸…ç†å¾Œçš„æ–‡æœ¬
    """
    if not text or not isinstance(text, str):
        return ""
    
    import re
    
    # ç§»é™¤ HTML æ¨™ç±¤
    text = re.sub(r'<[^>]+>', '', text)
    
    # çµ±ä¸€æ›è¡Œç¬¦è™Ÿ
    text = re.sub(r'\r\n|\r|\n', ' ', text)
    
    # è™•ç†é€£çºŒç©ºç™½
    text = re.sub(r'\s+', ' ', text)
    
    # ç§»é™¤é¦–å°¾ç©ºç™½
    text = text.strip()
    
    return text


def analyze_text_length_distribution(texts: list, tokenizer=None) -> dict:
    """
    åˆ†ææ–‡æœ¬é•·åº¦åˆ†ä½ˆï¼Œç”¨æ–¼è¨­å®š BERT çš„ max_length åƒæ•¸
    Args:
        texts: æ–‡æœ¬åˆ—è¡¨
        tokenizer: BERT åˆ†è©å™¨ï¼ˆå¯é¸ï¼‰
    Returns:
        é•·åº¦çµ±è¨ˆè³‡è¨Š
    """
    import numpy as np
    
    if tokenizer:
        # ä½¿ç”¨ BERT åˆ†è©å™¨è¨ˆç®— token é•·åº¦
        lengths = []
        for text in texts:
            tokens = tokenizer.encode(str(text), add_special_tokens=True)
            lengths.append(len(tokens))
    else:
        # ä½¿ç”¨å­—å…ƒé•·åº¦
        lengths = [len(str(text)) for text in texts]
    
    lengths = np.array(lengths)
    
    stats = {
        'count': len(lengths),
        'mean': float(np.mean(lengths)),
        'std': float(np.std(lengths)),
        'min': float(np.min(lengths)),
        'max': float(np.max(lengths)),
        'percentile_50': float(np.percentile(lengths, 50)),
        'percentile_95': float(np.percentile(lengths, 95)),
        'percentile_99': float(np.percentile(lengths, 99))
    }
    
    return stats


def load_bert_labeled_data(labeled_csv_path: str, input_csv_path: str) -> tuple:
    """
    è¼‰å…¥ç”¨æ–¼ BERT è¨“ç·´çš„æ¨™è¨»è³‡æ–™
    Args:
        labeled_csv_path: æ¨™è¨»çµæœæª”æ¡ˆè·¯å¾‘
        input_csv_path: åŸå§‹ç°¡è¨Šæª”æ¡ˆè·¯å¾‘
    Returns:
        (texts, labels, sms_ids)
    """
    import pandas as pd
    
    # è®€å–æ¨™è¨»çµæœ
    labeled_df = pd.read_csv(labeled_csv_path)
    
    # è®€å–åŸå§‹ç°¡è¨Šå…§å®¹
    input_messages = load_input_csv(input_csv_path)
    input_df = pd.DataFrame(input_messages)
    input_df.columns = ['sms_id', 'sms_body']
    
    # åˆä½µè³‡æ–™
    merged_df = pd.merge(labeled_df, input_df, on='sms_id', how='inner')
    
    # æ¸…ç†æ–‡æœ¬
    merged_df['sms_body'] = merged_df['sms_body'].apply(clean_text_for_bert)
    
    # ç§»é™¤ç©ºæ–‡æœ¬
    merged_df = merged_df[merged_df['sms_body'].str.len() > 0]
    
    texts = merged_df['sms_body'].tolist()
    labels = merged_df['label'].tolist()
    sms_ids = merged_df['sms_id'].tolist()
    
    return texts, labels, sms_ids


def save_bert_predictions(predictions: dict, probabilities: dict, output_path: str, classifier_type: str = "travel") -> None:
    """
    å„²å­˜ BERT æ¨¡å‹é æ¸¬çµæœï¼ˆåŒ…å«æ©Ÿç‡ï¼‰
    Args:
        predictions: é æ¸¬çµæœå­—å…¸ {id: label}
        probabilities: é æ¸¬æ©Ÿç‡å­—å…¸ {id: probability}
        output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        classifier_type: åˆ†é¡å™¨é¡å‹
    """
    import csv
    import os
    
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    try:
        with open(output_path, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            
            # æ ¹æ“šåˆ†é¡å™¨é¡å‹æ±ºå®šæ¬„ä½åç¨±
            if classifier_type == "name":
                writer.writerow(['sms_id', 'name_flg', 'probability'])
            else:  # travel
                writer.writerow(['sms_id', 'label', 'probability'])
            
            for sms_id in predictions:
                label = predictions[sms_id]
                prob = probabilities.get(sms_id, 0.0)
                writer.writerow([sms_id, label, prob])
        
        print(f"BERT é æ¸¬çµæœï¼ˆå«æ©Ÿç‡ï¼‰å·²å„²å­˜è‡³: {output_path}")
    except Exception as e:
        raise Exception(f"å„²å­˜ BERT é æ¸¬çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


def create_ensemble_submission(
    model_predictions: list,
    weights: list = None,
    max_submissions: int = 30000,
    output_path: str = None
) -> str:
    """
    å»ºç«‹å¤šæ¨¡å‹é›†æˆçš„æäº¤æª”æ¡ˆ
    Args:
        model_predictions: æ¨¡å‹é æ¸¬çµæœåˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ ç‚º (sms_id, label, probability)
        weights: æ¨¡å‹æ¬Šé‡åˆ—è¡¨
        max_submissions: æœ€å¤§æäº¤ç­†æ•¸
        output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
    Returns:
        è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
    """
    pass  # ä¿ç•™åŸæœ‰å¯¦ä½œ

def clean_text_for_bert(text: str) -> str:
    """
    ç‚º BERT æ¨¡å‹æ¸…ç†æ–‡æœ¬
    Args:
        text: åŸå§‹æ–‡æœ¬
    Returns:
        æ¸…ç†å¾Œçš„æ–‡æœ¬
    """
    import re
    import pandas as pd
    
    if not text or pd.isna(text):
        return ""
    
    # è½‰æ›ç‚ºå­—ç¬¦ä¸²ä¸¦å»é™¤å‰å¾Œç©ºç™½
    text = str(text).strip()
    
    # ç§»é™¤HTMLæ¨™ç±¤
    text = re.sub(r'<[^>]+>', '', text)
    
    # æ­£è¦åŒ–ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)
    
    # ç§»é™¤æ§åˆ¶å­—ç¬¦
    text = re.sub(r'[\r\n\t]', ' ', text)
    
    return text.strip()

def analyze_text_length_distribution(texts: List[str]) -> Dict:
    """
    åˆ†ææ–‡æœ¬é•·åº¦åˆ†å¸ƒ
    Args:
        texts: æ–‡æœ¬åˆ—è¡¨
    Returns:
        é•·åº¦çµ±è¨ˆå­—å…¸
    """
    import numpy as np
    
    lengths = [len(text) for text in texts]
    
    return {
        'mean': np.mean(lengths),
        'median': np.median(lengths),
        'std': np.std(lengths),
        'min': min(lengths),
        'max': max(lengths),
        'p95': np.percentile(lengths, 95),
        'p99': np.percentile(lengths, 99)
    }

def get_dataset_paths(category_type: str, base_dir: str = "data_game_2025/data/results/labled/stage2/train_data") -> Dict[str, str]:
    """
    ç²å–æŒ‡å®šåˆ†é¡é¡å‹çš„æ•¸æ“šé›†è·¯å¾‘
    Args:
        category_type: åˆ†é¡é¡å‹ ("name" æˆ– "travel")
        base_dir: åŸºç¤ç›®éŒ„è·¯å¾‘
    Returns:
        åŒ…å« train, val, test è·¯å¾‘çš„å­—å…¸
    """
    return {
        'train': os.path.join(base_dir, f"{category_type}_train_8000.csv"),
        'val': os.path.join(base_dir, f"{category_type}_val_8000.csv"),
        'test': os.path.join(base_dir, f"{category_type}_test_8000.csv")
    }
    
    # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é æ¸¬çµæœ
    all_predictions = {}
    
    for i, (predictions, model_name) in enumerate(model_predictions):
        weight = weights[i]
        
        for sms_id, label, prob in predictions:
            if sms_id not in all_predictions:
                all_predictions[sms_id] = {'probabilities': [], 'labels': []}
            
            all_predictions[sms_id]['probabilities'].append(prob * weight)
            all_predictions[sms_id]['labels'].append(label)
    
    # è¨ˆç®—åŠ æ¬Šå¹³å‡å’Œæœ€çµ‚é æ¸¬
    final_results = []
    for sms_id, data in all_predictions.items():
        avg_prob = np.mean(data['probabilities'])
        # ä½¿ç”¨å¤šæ•¸æŠ•ç¥¨æ±ºå®šæœ€çµ‚æ¨™ç±¤
        final_label = 1 if avg_prob > 0.5 else 0
        final_results.append((sms_id, final_label, avg_prob))
    
    # æŒ‰æ©Ÿç‡æ’åºä¸¦é¸å–å‰ N ç­†
    final_results.sort(key=lambda x: x[2], reverse=True)
    
    # åªä¿ç•™é æ¸¬ç‚ºæ­£ä¾‹çš„çµæœ
    positive_results = [(sms_id, label, prob) for sms_id, label, prob in final_results if label == 1]
    submission_results = positive_results[:max_submissions]
    
    # å»ºç«‹æäº¤æª”æ¡ˆ
    if output_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M")
        output_path = f"data_game_2025/data/results/ensemble_submission_{timestamp}.csv"
    
    # å„²å­˜çµæœ
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    import csv
    with open(output_path, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['sms_id', 'label'])
        
        for sms_id, label, _ in submission_results:
            writer.writerow([sms_id, label])
    
    print(f"é›†æˆæäº¤æª”æ¡ˆå·²ç”Ÿæˆ: {output_path}")
    print(f"æäº¤ç­†æ•¸: {len(submission_results)}")
    
    return output_path
